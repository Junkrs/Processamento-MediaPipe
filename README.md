# Processamento - MediaPipe ![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
Parte do Trabalho de Conclus√£o de Curso desenvolvido para minha gradua√ß√£o em Ci√™ncia da Computa√ß√£o na [UNESP](https://www.fc.unesp.br/#!/departamentos/computacao/cursos-de-graduao/bacharelado-em-ciencia-da-computacao/).
> [!IMPORTANT]
> Este reposit√≥rio corresponde ao processamento dos v√≠deos dos sinais de LIBRAS para gera√ß√£o dos arquivos JSON. Para a renderiza√ß√£o e anima√ß√£o dos esqueletos dos avatares, veja o conte√∫do disponibilizado [neste reposit√≥rio](https://github.com/Junkrs/Ambiente---TCC). :leftwards_arrow_with_hook:

## üîµ Introdu√ß√£o ao Projeto

Este trabalho tem como objetivo a cria√ß√£o de um sistema que capture e redirecione os movimentos da L√≠ngua Brasileira de Sinais (LIBRAS) para animar esqueletos de avatares 3D, usando ferramentas de intelig√™ncia artificial e vis√£o computacional. Utilizando o [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/guide) para rastreamento de pontos chaves em v√≠deos que contenham sinais de LIBRAS, foram animados esqueletos de avatares 3D dentro do Unity.

A monografia do projeto completo est√° dispon√≠vel no [reposit√≥rio institucional da UNESP](https://hdl.handle.net/11449/258220).

## üîµ Como Usar

Existem 3 pastas importantes e 1 arquivo .py, que √© respons√°vel por todo o processamento.

- Pastas:
1. _Holistic_Models_: Essa pasta cont√©m os modelos do MediaPipe. Neste caso, temos 3 vers√£o do MediaPipe Pose, que foi utilizado no projeto.
2. _saida_processamento_: Essa pasta √© o endere√ßo de sa√≠da dos v√≠deos processados e tamb√©m dos arquivos JSON gerados com cada v√≠deo.
3. _videos UFPE (V-LIBRASIL)_: J√° esta pasta √© onde estar√£o os v√≠deos originais para o processamento. Ela est√° escrita dessa forma pois o _dataset_ escolhido no TCC foi o [V-LIBRASIL](https://libras.cin.ufpe.br/).
<br>

- C√≥digo fonte:
  O c√≥digo fonte √© divido em fun√ß√µes que ir√£o anotar, fazendo a ilustra√ß√£o dos _landmarks_ e depois ir√£o gerar o arquivo JSON, com as coordenadas de cada _landmark_ em cada _frame_ do v√≠deo. Alguns dos trechos de c√≥digo que s√£o importantes para futuras modifica√ß√µes s√£o:
  
  1. Modelo escolhido do MediaPipe (linha 80):
     
     ```
      model = holistic_models.get('Full')
     ```
  2.  Pontos do modelo escolhido do MediaPipe (linha 145):
     
       ```
       point_names = {...}
       ```
  3. Pontos que devem ser ignorados (Ppcional) (Linha 183):
     
     ```
      ignored_points = {...}
     ```
  4. Formato de sa√≠da dos dados no arquivo JSON (Linha 196):
     
     ```
      formatted_pose_landmarks.append(
       {
         f"ponto_{num} - {point_name}": {
           "x": landmark.x,
           "y": landmark.y,
           "z": landmark.z,
         }
       }
     )
     ```
Uma vez processados, os arquivos JSON da pasta de sa√≠da podem ser inseridos na [segunda parte](https://github.com/Junkrs/Ambiente---TCC) deste projeto. Desde que o formato esteja adequeado.

#### Qualquer d√∫vida, fique a vontade para entrar em contato comigo :D
